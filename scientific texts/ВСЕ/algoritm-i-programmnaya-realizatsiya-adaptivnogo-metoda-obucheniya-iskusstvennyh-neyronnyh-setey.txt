*26*
Алгоритм и программная реализация адаптивного метода обучения искусственных
нейронных сетей.
                       В. Б.Лила РГСУ, Ростов-на-Дону

     Задача обучения искусственной нейронной сети (ИНС) может рассматриваться как задача оптимизации, при этом основная проблема заключается в выборе среди разнообразных оптимизационных методов наиболее подходящего[1,2,3,4,5].
     В основном все оптимизационные алгоритмы можно разбить на три категории:
         ? методы случайного поиска;
         ? методы стохастического градиентного спуска;
         ? градиентные методы.
     К группе градиентных методов относятся следующие методы:
         ? метод градиентного спуска;
         ? метод тяжелого шарика;
         ? методы сопряженных градиентов.
     Выбор в пользу градиентных методов обоснован тем, что, как правило, в задачах
обучения критерий обучения можно выразить в виде дифференцируемой функции от
весов нейронной сети. Тем не менее, неопределенность выбора метода обучения
сохраняется. В автоматизированных системах нейро-сетевого программирования следует
стремиться к сокращению неопределенности, которая присуща этим технологиям.
Неопределенность в выборе алгоритма обучения в некоторой степени устраняется в
предлагаемом ниже адаптивном методе обучения.
     Общий анализ        градиентных методов обучения нейронных сетей позволяет
утверждать, что любой из этих методов можно представить как частный случай
предлагаемого алгоритма.
     Общую формулу для изменения весов выразим формулой (1), где вектор p k задает 
направление движения, а ? k - размер шага на k-ой итерации.
     Формулу расчета вектора p k выразим следующим образом, где вектор p k задает 
направление движения; g j - направление антиградиента на j-ой
итерации; ? i - коэффициент, определяющий вес i-го градиента; m определяет кол-во
запоминаемых градиентов; k – порядковый номер текущей итерации.
     Градиентный метод обучения из формулы 2 получается при m ? 0 . А методы
сопряженных градиентов[1], которые наиболее часто употребляются при обучении
нейронных сетей, получаются путем суммирования всех предыдущих направлений (при
m ? ? ).
     Таким образом, предлагаемый адаптивный алгоритм является более гибким
решением при обучении нейронных сетей.
     Общий алгоритм адаптивного метода:
         1. Начало
         2. Выбираем стартовую точку с некоторыми координатами (x0;y0 ;...).
         3. Проверяем критерий остановки (число итераций, средняя квадратическая
            ошибка и др)
         4. Вычисляем антиградиент в текущей точке g j (на первой итерации стартовая
           точка).
         5. Заносим текущее направление в стек направлений.
         6. Считаем вектор направления по формуле 2.
	 7. Перемещаемся по вычисленному вектору p k в новую точку.
         8. Возвращаемся в шаг 2. Если критерий остановки положителен, то
            заканчиваем алгоритм, если нет – переходим к шагу 3.
         9. Конец алгоритма. Имеем точку, близкую к минимуму функции.

     Рассмотрим пример использования такого подхода при оптимизации функции
Розенброка [2].
     Вычисление минимума функции Розенброка считается трудной проблемой для
итерационных методов поиска минимума.
     Для проведения экспериментов было разработано программное обеспечение на
платформе Microsoft .NET на языке C#.
     Проведены эксперименты, из одной стартовой точки с координатами (-1.5; 3.5), с
изменением параметров: ?, m, ?.
     Для градиентного спуска (m=0), при оптимальном ?=0.005                 значение целевой
функции - 0.30103179 достигается на 121-ой итерации. Движение и изменение значений
функции изображено на рис. 2.
     Для метода тяжелого шарика[1] , который также является частным случаем
адаптивного метода, при m = 1, ?=0.9 и ?=0.005, значение целевой функции 0.12836612
достигается на 52-ой итерации. Результаты вычислений представлены на рис. 3.
     Для адаптивного алгоритма с параметрами ?=0.0004, m = 2, ? = (0.9,0.081) значение
целевой функции - 0.04807101 получено на 35-ой итерации. Результаты расчетов
представлены на рис. 4.
     Таким образом, для сложной функции Розенброка (3) адаптивный алгоритм
оказался лучшим, по сравнению с классическими методами: градиентного спуска и
тяжелого шарика.
     Применение адаптивного алгоритма для обучения нейронных сетей требует
настройки параметров: m, ? и h . В связи с этим предлагается следующий адаптивный
метод обучения ИНС, на основе адаптивного алгоритма.
     Обозначим через ИНС 1 – основную нейронную сеть, которая будет обучаться на
реальных данных - обучающей выборке 1 с помощью настроенного адаптивного
алгоритма. Для настройки параметров алгоритма обучения используется ИНС 2,
топологически вложенная в ИНС 1 и обучающая выборка 2, полученная путем случайного
отбора некоторых примеров обучающей выборки 1. Для сокращения числа настраиваемых
параметров воспользуемся представлением.

     Контроллер, обучающий ИНС 2 в автоматическом режиме на примере обучающей
выборки 2, подбирает оптимальные параметры для адаптивного алгоритма. Критерием
при этом является минимум числа итераций необходимых для достижения минимума.
     После чего оптимальные параметры обучения передаются в ИНС 1 для обучения на
реальных данных.

     Предложенный метод обучения сводит к минимуму вмешательство человека в
обучение ИНС, что делает его привлекательным. Поскольку не каждый пользователь
ИНС-технологий владеет знаниями в области методов оптимизации. Кроме этого метод
является гибким и настраиваемым на обучающую выборку методом обучения.
