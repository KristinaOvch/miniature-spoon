*24*
УДК 123.456, 789.012

ПАРАЛЛЕЛЬНЫЙ АЛГОРИТМ РЕШЕНИЯ ЗАДАЧ
ДИНАМИКИ ЗАРЯЖЕННЫХ ЧАСТИЦ С УЧЕТОМ
БАЛАНСИРОВКИ ВЫЧИСЛИТЕЛЬНОЙ НАГРУЗКИ1
Е.А. Берендеев, М.А. Боронина, В.Д. Корнеев
Рассмотрены задачи динамики встречных пучков заряженных частиц в ускорителях и
динамики плазменных электронов в ловушке с инверсными магнитными пробками и мультипольными магнитными стенками. Модели построены на основе метода частиц в ячейках.
Такие задачи требуют большого объема вычислений и могут быть решены только с применением мощных суперЭВМ. Для равномерной и полной загрузки вычислительных узлов выполнена модификация эйлерово-лагранжевой декомпозиции в случае существенно неравномерного распределения частиц по пространству и по времени.
Ключевые слова: метод частиц в ячейках, параллельные алгоритмы, балансировка
нагрузки, физика плазмы, встречные пучки, ускорители частиц.

Введение
Данная работа посвящена созданию и исследованию параллельных алгоритмов для
моделирования некоторых задач физики плазмы методом частиц.
В частности, для моделирования динамики встречных пучков заряженных частиц в
современных ускорителях с учетом высоких значений релятивистского фактора частиц
и трехмерности задачи. При этом требуется проводить расчеты на сетках порядка
512x512x512 с количеством частиц в пучке ~1010. В основе существующих стандартных
численных моделей и алгоритмов, направленных на решение задач с высокими релятивистскими факторами (103 и более), лежит разделение пучка вдоль оси коллективного
движения на слои частиц. При этом взаимодействие встречных пучков сводится к попарному взаимодействию слоев: частицы одного слоя через поле поперечных сил влияют
на динамику частиц другого. К настоящему времени созданы и параллельные алгоритмы решения задач динамики заряженных частиц в ускорителях. Обычно каждый процессор отвечает за свой слой частиц или за несколько слоев, в некоторых случаях имеется динамическая балансировка загрузки процессоров. Но используемый квазитрехмерный подход даже с учетом распараллеливания затрудняет учет продольных
эффектов при критически высокой плотности пучка, когда за короткий промежуток
времени пучок деформируется или даже разрушается, а также возникают трудности
моделирования таким способом при сравнительно большом угле пересечения пучков
(~20 мрад). Существенно нелинейное распределение плотности пучка (по гауссу с условием фокусировки), значительное изменение формы пучка при пролете через область
(форма песочных часов), требования к количеству частиц пучка при заданной трехмерной сетке [1] приводят к необходимости создания кода с балансировкой нагрузки процессоров.

Аналогичные требования к размеру сетки и количеству частиц предъявляет задача
динамики плазменных электронов в ловушке с инверсными магнитными пробками и
мультипольными магнитными стенками. Величина магнитного поля внутри и на торцах
ловушки существенно отличается. При этом, чтобы точно воспроизвести движение частиц в сильном магнитном поле, необходимо использовать достаточно мелкий временной
шаг. В слабом магнитном поле величину временного шага можно увеличить и тем самым ускорить вычисления, т.к. траектории частиц в этом случае достаточно гладкие.
Однако при построении параллельного алгоритма необходимо учитывать, что за реальный промежуток времени разные частицы совершают движение за разное количество
временных шагов. Таким образом, объем вычислений для каждой частицы будет разный, даже если частиц на каждом процессоре поровну.
Рассмотренные задачи обладают существенной неоднородностью распределения частиц как по пространству, так и по времени. Поэтому нашей целью явилось создание
параллельного алгоритма с балансировкой нагрузки на процессоры, обладающего хорошей масштабируемостью для возможности проведения численных экспериментов с использованием высокого количества частиц. В работе описана постановка задачи, используемые модели и методы, а также описание специфики распараллеливания и сравнение с другими работами мирового уровня. Представлена блок-схема алгоритма и результаты исследований его для обеих задач: время счета, эффективность распараллеливания, ускорение. Приведены примеры использования алгоритма для проведения математического моделирования соответствующих физических явлений.

1. Постановка задачи и методы решения
В задаче рассматривается движение частиц, которое происходит в вакууме в самосогласованных электромагнитных полях с учетом внешней составляющей электромагнитного поля. При этом модельный пучок представляет собой набор достаточно большого количества частиц с соответствующими параметрами [2, 3]. Такое движение моделируется с помощью кинетического уравнения Власова [4] для функции распределения частиц с положительным.

Уравнения характеристик уравнения Власова совпадают с уравнениями движения
частиц.

Система уравнений Власова-Максвелла решается методом частиц в ячейках, с использованием ядра PIC. Так как в уравнения входят первые производные и по времени,
и по пространству, то применяется схема с перешагиванием [5].
Компоненты магнитного поля
вычисляются в центрах ребер ячеек, образованных пространственной сеткой, а значения электрического поля
динах граней этих ячеек (рис. 1).
При этом электрическое поле вычисляется в целые моменты времени n?, а магнитное поле – в дробные моменты времени n(? + 1/2). Токи вычисляются в тех же точках
пространства, что и электрическое поле, но по времени берутся в дробных узлах. Значения импульсов частиц также вычисляются в моменты времени n(? + 1/2), а координаты
– в целые. При этом все производные, участвующие в уравнениях, как по времени, так и
по пространству, записываются через центральные разности, что обеспечивает при использовании этой схемы второй порядок по времени и по пространству.
Сила Лоренца определяется линейной интерполяцией по каждому направлению
сил электромагнитных полей, вычисленных в каждом из ближайших к частице восьми
узлов.
Уравнения Максвелла решаются по следующим конечно-разностным схемам.
Токи вычисляются по координатам частиц с использованием ядра PIC–метода по
схеме, предложенной Вилласенором и Бунеманом [6]. Такой метод вычисления токов
позволяет автоматически удовлетворить разностному уравнению неразрывности и, следовательно, точно выполнить разностный закон Гаусса. Это значительно уменьшает
ошибки аппроксимации и делает алгоритм более устойчивым.

2. Параллельная реализация алгоритма
Метод частиц, созданный еще в 60-х годах 20 века, в настоящее время является развитым и широко применяемым методом для решения задач бесстолкновительной динамики частиц, ежегодные публикации демонстрируют постоянный интерес к этому методу.
В качестве приложения описанного выше алгоритма используются две задачи. Одна
из них — получение мощных нейтральных пучков для установок управляемого термоядерного синтеза. Наиболее эффективным методом получения таких пучков является
нейтрализация пучков отрицательных ионов в плазменной ловушке — мишени. В ИЯФ
СО РАН предложена линейная ловушка с обратным магнитным полем [7]. Для ограничения радиальных потерь плазмы используются мультипольные магнитные стенки
кольцевой геометрии. В осесимметричной ловушке с кольцевым магнитным полем отсутствует азимутальный компонент поля, а так же отсутствует стационарное азимутальное электрическое поле. Оценка и минимизация потерь плазмы в широко апертурные проходные отверстия в торцах, в которых находятся инверсные магнитные пробки,
а также через цилиндрические мультипольные магнитные стенки ловушки на ее вакуумную камеру, может быть исследована только с помощью математического моделирования. При этом, наиболее полно динамика плазменных электронов может быть описана уравнением Больцмана.

Здесь St{ f + ,? } — функция, описывающая следующие физические процессы:
• ионизация атома водорода,
• ионизация и диссоциация молекулы H2
2+
• диссоциативное возбуждение и диссоциативная рекомбинация H
• диссоциативная рекомбинация D2+
• перезарядка протонов на атомах водорода.
Решение уравнения Больцмана (9) можно свести к решению уравнения Власова (1)
и корректировке траекторий частиц с учетом рассеяния, используя методы МонтеКарло.

В настоящей работе рассеяние не учитывается, оно будет являться предметом дальнейшего исследования. Таким образом, решается система, описанная в параграфе 1, но в
цилиндрической системе координат. Задача рассматривается в двумерной R-Z геометрии. Особенностью данной задачи является движение частиц под воздействием магнитных полей с большими градиентами. Для ускорения расчета траекторий частиц использовался динамический шаг по времени, обеспечивающий изменение напряженности магнитного поля не более 20% за один шаг.
Другой вариант использования алгоритма — трехмерное численное моделирование
эффектов встречи пучков частиц с критическими параметрами (в частности, с высокими значениями релятивистских факторов и с предельно высокой плотностью зарядов) в
ускорителях. В этом случае пучок может не только сильно сжиматься, но и разрушаться, поэтому требуется исследование устойчивости пучков. С точки зрения математического моделирования в задачах с большими значениями релятивистского фактора (? ~3
10 ) имеется существенное отличие от задач c малыми значениями (? ~ 5). Известно, что
поле движущейся заряженной частицы в лабораторной системе координат вытягивается
в ? раз поперек оси движения и сокращается в ?2 раз вдоль этой оси. Так, например,
при значениях релятивистского фактора ? ~ 103 отношение поперечных размеров к продольному, на которых поля близки по абсолютной величине, составляет ~ 109, и использование традиционных путей решения становится невозможным.
В основе существующих стандартных численных моделей и алгоритмов, направленных на решение задач с высокими релятивистскими факторами (103 и более), лежит
разделение пучка вдоль оси коллективного движения на слои частиц. При этом взаимодействие встречных пучков сводится к попарному взаимодействию слоев: частицы одного слоя через поле поперечных сил влияют на динамику частиц другого (коды GuineaPig, ODYSSEUS) [9, 10]. В большинстве существующих программных кодов, основанных
на данном подходе, для получения поперечного поля используются либо формулы Бассетти – Ерскине [11], либо двумерное уравнение Пуассона с использованием быстрого
преобразования Фурье (при этом граничные условия вычисляются через двумерную
функцию Грина). К настоящему времени созданы и параллельные алгоритмы решения
задач динамики заряженных частиц в ускорителях (коды COMBI, IMPACT, Beam
Beam 3D) [12–13]. Обычно каждый процессор отвечает за свой слой частиц или за несколько слоев, в некоторых случаях имеется динамическая балансировка загрузки процессоров. Но используемый квазитрехмерный подход затрудняет учет продольных эффектов при критически высокой плотности пучка, когда за короткий промежуток времени пучок деформируется или даже разрушается, а также возникают трудности моделирования таким способом при сравнительно большом угле пересечения пучков.
При этом интерес представляет изучение эффектов на расстоянии порядка дисперсии в поперечном направлении. Сама эта величина является небольшой, за счет фокусировки пучка (hour-glass эффект) его размеры в поперечном направлении сильно увеличиваются. Поэтому необходимо проводить исследования эффектов при больших размерах области на малых расстояниях от ее центра, то есть брать достаточно много и
мелких пространственных шагов в поперечном направлении, что также приводит к
необходимости использования параллельного алгоритма. Кроме того, временной шаг
требуется выбирать не из соображений сходимости, так как он уже достаточно мал, а из
соображений устойчивости метода. Таким образом, уменьшение сетки в поперечном
направлении ведет не только к увеличению количества действий, связанных с количеством узлов сетки, но и к увеличению количества шагов программы.
Повышение точности расчетов методом части может быть достигнуто увеличением
количества узлов пространственной сетки, однако такой способ не всегда возможно
применять ввиду ограниченности памяти ЭВМ. Кроме того, для решений методом частиц характерны численные шумы, «самосила» и другие негативные эффекты, связанные с введением пространственной сетки. Одним из способов уменьшения влияния этих
эффектов является увеличение количества частиц в ячейке. Кроме того, при увеличении
количества узлов по каждому направлению, количество частиц также должно быть
увеличено. Поэтому единственным вариантом достижения требуемых количественных
характеристик является распараллеливание.
Поэтому для распараллеливания алгоритма для обеих задач выбран метод декомпозиции. Наиболее простая декомпозиция представляет собой разделение области на полосы в соответствующей системе координат. Для наибольшей эффективности вычислений
с целью уменьшить число межпроцессорных коммуникаций необходимо модифицировать имеющиеся алгоритмы с учетом специфики решаемых задач [14]. Модификация
представляет собой эйлеро-лагранжеву декомпозицию [15].

С каждой подобластью-полосой связана группа процессоров, частицы в каждой подобласти разделены между всеми процессорами группы, поэтому, чем больше частиц в
определенной области – тем больше процессоров в соответствующей группе. Выбором
количества процессоров в группе можно добиться необходимого максимального количества частиц в процессоре. Схема распараллеливания приведена на рисунке
2. Таким образом, в случае неравномерного распределения частиц по пространству, на
каждую полосу приходится количество процессоров пропорциональное количеству частиц в полосе.
Если же частицы движутся с различным шагом по времени, то частицы распределяются между процессорами полосы не равномерно. Поскольку величина магнитного
поля в пределах ячейки существенно не меняется, можно для каждой ячейки ввести
поправочный коэффициент, характеризующий величину шага по времени для частиц
данной ячейки. Чем больше магнитное поле - тем меньше поправочный коэффициент.
Исходя из этих рассуждений, для того, чтобы вычислительная нагрузка на процессоры
была сбалансирована, число процессоров N pg в группе оценивается следующим образом:
Здесь N j — число частиц в ячейке j, t j — поправочный коэффициент, N p — общее число процессоров.
На рисунке 3 приведена блок-схема алгоритма. Здесь белым цветом обозначены
блоки программы, выполняемые каждым процессором, серым — нулевым процессором,
и темно-серым –— главными процессорами каждой группы. Индексом g обозначены
значения функция, вычисленные каждым процессором группы, которые далее используются главным процессором группы для получения соответствующей суммарной функции. Обозначение send описывает пересылку значений каждым процессором группы
главному процессору группы, а обозначение recv — принятие значений главным процессором группы от всех процессоров его группы, обозначение bcast — отправку значений
главного процессора всем процессорами группы и соответствующее получение их. В
случае ловушки требуются дополнительные процедуры по добавлению/удалению частиц. Этого не требуется в случае встречных пучков, но взамен необходимо вычислять
граничные условия на каждом шаге по времени. Поэтому переход к каждому из блоков
показан пунктирной линией.
После вычисления начальных условий выполняется цикл по времени до достижения
нужного момента времени. В этом цикле требуется решать уравнения Максвелла, и
каждая группа решает их только в своей подобласти. Уравнение Максвелла для вычисления электрического поля решается только главными процессорами группы, для этого
производится сбор необходимых данных — плотностей и токов, а также обмен значениями на смежных гранях. За счет использования дополнительных узлов и равных значений электрического поля во всех процессорах магнитное поле вычисляется каждым
процессором без пересылок. Также группы обмениваются частицами, оказавшимися в
соответствующей подобласти. Такое распараллеливание по пространству и по частицам
позволяет решить проблему существенно неравномерного распределения плотности частиц в области, обеспечивая равномерную загрузку процессоров внутри группы.

3. Эффективность параллельного алгоритма
Описанный алгоритм был протестирован на некоторых характерных примерах движения ультрарелятивистского пучка. В частности, предполагается, что пучок моноэнергетических электронов двигается вдоль оси z. Плотность частиц распределена по закону
Гаусса со следующими параметрами фокусировки в безразмерных величинах ?x = ?y =
-7
5•10 , ?x = ?y=0,1, ?z=0,1, где ?x and ?y горизонтальный и вертикальный эмиттансы
пучка, ?x, ?y — соответствующие значения бета-функции, ?z — размер пучка по оси z.
3
8
Релятивистский фактор ? = 6,85·10 , заряд пучка Q= 2,63·10 e. Размер области в без?2
размерных величинах Lx = Ly = 10 , Lz = 1. Размер сетки 120х120х120 узлов, времен-5
4
ной шаг 10 , количество шагов 3•10 .
Все численные эксперименты проводились на кластере Сибирского Суперкомпьютерного Центра (ИВМиМГ), с
576-ю 4-ядерными процессорами Intel Xeon
Е5450/E5540/X5670.
На рис. 4 показана зависимость поля Ex от координаты z в плоскости (x,z) при разном количестве частиц 105, 106, 107 в конечный момент времени в безразмерных величинах.
Из рисунков видно, что при увеличении количества частиц остаточное поле, следующее за пучком, уменьшается, сглаживается. Рисунки также демонстрируют сходимость метода. Если же количество частиц в ячейке недостаточно, то высокие значения
шумов в методе частиц приводят к неустойчивостям, затрудняющим исследование физических эффектов.
В табл. 1 представлены время расчета пролета пучка через область и максимальное
количество частиц в процессоре, как в начальный момент (предпоследняя колонка), так
и в течение всего расчета (последняя колонка). Таблица демонстрирует преимущество
добавочных процессоров в группах, соответствующих высокой плотности частиц. Для
линейной декомпозиции с количеством процессоров Np=Npg=12 время вычислений составило 84 с, и 98 с — для Np=Npg=6, но время расчета намного меньше, когда используется всего 12 процессоров в 6 группах. Таким образом, намного эффективнее

Параллельный алгоритм решения задач динамики заряженных частиц с учетом...

но далее обмен трехмерными массивами сеточных значений занимает все больше времени, а поэтому эффективность распараллеливания падает.
Аналогичный результат получен и для параметров Np =20 на сетке 120х120х60, чис7
ло частиц 10 , время расчета 1000 шагов по времени 67 с (рис. 6.). При этом провалы на
графике связаны с несимметричным распределением процессоров в группах, поэтому
некоторых из них простаивают. Из численных экспериментов следует, что оптимальное
соотношение процессоров Npg и Np достигается, когда число групп Npg примерно в 7 раз
меньше размера сетки вдоль направления распараллеливания, а общее число процессоров Np больше количества групп Npg примерно в 3–5 раз. В случае большой сетки эффективность падает за счет пересылок копий сеточных значений, тем не менее – это
единственный способ проведения расчетов с таким большим количеством частиц и с таким мелким шагом по времени. Т.к. разумный минимум узлов в процессоре Ny равен 4
(2 узла + 2 вспомогательных узла), то параметры расчета ограничены только ресурсами
ЭВМ для массивов размером ~4NxNz, соответствующих сеточным значениям подобласти.
В качестве второй задачи рассматривается динамика плазменных электронов в ловушке мишени. Поскольку задача двумерна, объем пересылаемых данных существенно
ниже. В этом случае возможно добиться достаточно высокой масштабируемости алгоритма.
Характеристики задачи: температура плазмы 5 эВ, размер области 6,1 см х 1,2 см.
Сетка 4096?128 узлов, общее число модельных частиц 5 242 880 000. Расчеты проводились на суперкомпьютере «Ломоносов» с использованием до 8192 процессорных ядер.
В таблице 2 представлено время расчета одного шага (в секундах) при использовании различного количества процессорных ядер, а также полученное при этом ускорение.
В связи с большим объемом требуемой оперативной памяти, масштабируемость рассматривается относительно 1024 процессорных ядер.
Рисунок показывает, что инверсные магнитные пробки на торцах
достаточно хорошо удерживают ловушки и границе инверсных пробок. Для
количественного описания потерь плазму в ловушке, в то же время присутствуют

Заключение
Реализован параллельный алгоритм решения задач динамики плазмы, основанный
на модификации эйлерово-лагранжевой декомпозиции в случае неравномерного распределения частиц по пространству и времени. В качестве примеров применения разработанного алгоритма были рассмотрены две задачи - динамики встречных ультрарелятивистских пучков заряженных частиц в ускорителях и динамики плазменных электронов
в ловушке с инверсными магнитными пробками и мультипольными магнитными стенками. Для задачи динамики встречных пучков уменьшение остаточного поля, следующего за пучком, с увеличением счетных параметров подтвердило сходимость метода.
Для задачи ловушки получены траектории движения плазменных электронов и точки
выхода плазмы на стенки ловушки. Применение алгоритма в обоих случаях позволило
достичь высокой масштабируемости и увеличить число частиц в расчетах до 1010. Использование большого количества частиц даст возможность проводить расчеты с другими конфигурациями полей и частиц, и достигать большей точности решений.
Работа выполнена при поддержке грантов РФФИ 14-01-00392, 14-01-31220, а
также МИП СО РАН №105 и №130.
